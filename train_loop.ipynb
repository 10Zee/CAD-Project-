{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMrx2KPah2SaFWWzHTB6ab",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/10Zee/CAD-Project-/blob/main/train_loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7FyR2s33Ikz"
      },
      "outputs": [],
      "source": [
        "!pip install timm\n",
        "import os\n",
        "from timm import create_model\n",
        "from fastai.vision.all import *\n",
        "import torch.utils.data as TUD\n",
        "from fastai.vision.learner import _update_first_layer\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from torch import from_numpy\n",
        "from torch import nn\n",
        "\n",
        "from torch.optim import Adam\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "from torch.optim import Adam\n",
        "os.chdir('/content/drive/MyDrive/Colab')\n",
        "%run 'data_prep.ipynb'\n",
        "%run 'training_eval.ipynb'\n",
        "%run 'train_ABMIL.ipynb'\n",
        "import torchvision.transforms.functional as TF\n",
        "env = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "torch.backends.cudnn.benchmark = True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WzRx19Gq3M2U",
        "outputId": "ca6739a1-6983-4dfc-dc9a-66fd5ba51247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BagOfImagesDataset(TUD.Dataset):\n",
        "\n",
        "    def __init__(self, filenames, ids, labels, normalize=True):\n",
        "        self.filenames = filenames\n",
        "        self.labels = from_numpy(labels)\n",
        "        self.ids = from_numpy(ids)\n",
        "        self.normalize = normalize\n",
        "        #self.imsize = imsize\n",
        "        # Normalize\n",
        "        if normalize:\n",
        "            self.tsfms = T.Compose([\n",
        "                T.RandomVerticalFlip(),\n",
        "                T.RandomHorizontalFlip(),\n",
        "                T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "                T.RandomAffine(\n",
        "                    degrees=(-20, 20),  # Random rotation between -10 and 10 degrees\n",
        "                    translate=(0.05, 0.05),  # Slight translation\n",
        "                    scale=(0.95, 1.05),  # Slight scaling\n",
        "                ),\n",
        "                T.ToTensor(),\n",
        "\n",
        "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        else:\n",
        "            self.tsfms = T.Compose([\n",
        "                T.ToTensor(),\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(torch.unique(self.ids))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        where_id = self.ids == index\n",
        "        files_this_bag = self.filenames[where_id]\n",
        "\n",
        "        # Define a transform to resize images to a common size (e.g., 672x959)\n",
        "        #resize_transform = T.Compose([T.Resize((672, 959)), T.ToTensor()])\n",
        "\n",
        "        # Modify the data loading part of your code\n",
        "        #data = torch.stack([\n",
        "            #self.tsfms(resize_transform(Image.open(fn).convert(\"RGB\"))) for fn in files_this_bag\n",
        "        #]).cuda()\n",
        "        #(T.Resize((672, 959))\n",
        "        data = torch.stack([\n",
        "            self.tsfms(T.Resize((672, 959))(Image.open(fn).convert(\"RGB\"))) for fn in files_this_bag\n",
        "        ]).cuda()\n",
        "        #data = torch.stack([\n",
        "          #  self.tsfms(Image.open(fn).convert(\"RGB\")) for fn in files_this_bag\n",
        "        #]).cuda()\n",
        "\n",
        "        labels = self.labels[index]\n",
        "\n",
        "        return data, labels\n",
        "\n",
        "    def show_image(self, index, img_index=0):\n",
        "        # Get the transformed image tensor and label\n",
        "        data, labels = self.__getitem__(index)\n",
        "\n",
        "        # Select the specified image from the bag\n",
        "        img_tensor = data[img_index]\n",
        "\n",
        "        # If the images were normalized, reverse the normalization\n",
        "        if self.normalize:\n",
        "            mean = torch.tensor([0.485, 0.456, 0.406]).to(img_tensor.device)\n",
        "            std = torch.tensor([0.229, 0.224, 0.225]).to(img_tensor.device)\n",
        "            img_tensor = img_tensor * std[:, None, None] + mean[:, None, None]  # Unnormalize\n",
        "\n",
        "        # Convert the image tensor to a PIL Image\n",
        "        img = TF.to_pil_image(img_tensor.cpu())\n",
        "\n",
        "        # Display the image and label\n",
        "        plt.imshow(img)\n",
        "        plt.title(f'Label: {labels}')\n",
        "        plt.axis('off')  # Hide the axis\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def n_features(self):\n",
        "        return self.data.size(1)\n",
        "\n",
        "def collate_custom(batch):\n",
        "    batch_data = []\n",
        "    batch_bag_sizes = [0]\n",
        "    batch_labels = []\n",
        "\n",
        "    for sample in batch:\n",
        "        batch_data.append(sample[0])\n",
        "        batch_bag_sizes.append(sample[0].shape[0])\n",
        "        batch_labels.append(sample[1])\n",
        "\n",
        "    out_data = torch.cat(batch_data, dim = 0).cuda()\n",
        "    bagsizes = torch.IntTensor(batch_bag_sizes).cuda()\n",
        "    out_bag_starts = torch.cumsum(bagsizes,dim=0).cuda()\n",
        "    out_labels = torch.stack(batch_labels).cuda()\n",
        "\n",
        "    return (out_data, out_bag_starts), out_labels"
      ],
      "metadata": {
        "id": "sR3hC3HSBDfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function is used to cut off the head of a pretrained timm model and return the body\n",
        "def create_timm_body(arch:str, pretrained=True, cut=None, n_in=3):\n",
        "    \"Creates a body from any model in the `timm` library.\"\n",
        "    model = create_model(arch, pretrained=pretrained, num_classes=0, global_pool='')\n",
        "    _update_first_layer(model, n_in, pretrained)\n",
        "    if cut is None:\n",
        "        ll = list(enumerate(model.children()))\n",
        "        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n",
        "    if isinstance(cut, int): return nn.Sequential(*list(model.children())[:cut])\n",
        "    elif callable(cut): return cut(model)\n",
        "    else: raise NameError(\"cut must be either integer or function\")"
      ],
      "metadata": {
        "id": "yrF8AoG7BIHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ABMIL_aggregate(nn.Module):\n",
        "\n",
        "    def __init__(self, nf, num_classes, pool_patches = 3, L = 128):\n",
        "        super(ABMIL_aggregate,self).__init__()\n",
        "        self.nf = nf\n",
        "        self.num_classes = num_classes # two for binary classification\n",
        "        self.pool_patches = pool_patches # how many patches to use in predicting instance label\n",
        "        self.L = L # number of latent attention features\n",
        "\n",
        "        self.saliency_layer = nn.Sequential(\n",
        "            nn.Conv2d( self.nf, self.num_classes, (1,1), bias = False),\n",
        "            nn.Sigmoid() )\n",
        "\n",
        "        self.attention_V = nn.Sequential(\n",
        "            nn.Linear(self.nf, self.L),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.attention_U = nn.Sequential(\n",
        "            nn.Linear(self.nf, self.L),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.attention_W = nn.Sequential(\n",
        "            nn.Linear(self.L, self.num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        # input is a tensor with a bag of features, dim = bag_size x nf x h x w\n",
        "\n",
        "        h = h.permute(0, 3, 1, 2)  # Now h has shape [1, 512, 10, 64]\n",
        "\n",
        "        saliency_maps = self.saliency_layer(h)\n",
        "        map_flatten = saliency_maps.flatten(start_dim = -2, end_dim = -1)\n",
        "        selected_area = map_flatten.topk(self.pool_patches, dim=2)[0]\n",
        "        yhat_instance = selected_area.mean(dim=2).squeeze()\n",
        "\n",
        "        # gated-attention\n",
        "        v = torch.max( h, dim = 2).values # begin maxpool\n",
        "        v = torch.max( v, dim = 2).values # maxpool complete\n",
        "        A_V = self.attention_V(v)\n",
        "        A_U = self.attention_U(v)\n",
        "        attention_scores = nn.functional.softmax(\n",
        "            self.attention_W(A_V * A_U).squeeze(), dim = 0 )\n",
        "\n",
        "        # aggreate individual predictions to get bag prediciton\n",
        "        yhat_bag = (attention_scores * yhat_instance).sum(dim=0)\n",
        "\n",
        "        return yhat_bag, saliency_maps, yhat_instance, attention_scores\n",
        "\n",
        "class EmbeddingBagModel(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, aggregator, num_classes=1):\n",
        "        super(EmbeddingBagModel,self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.aggregator = aggregator\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input[0]\n",
        "        bag_sizes = input[1]\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(h.size(0), -1, h.size(1))\n",
        "\n",
        "        num_bags = bag_sizes.shape[0]-1\n",
        "        logits = torch.empty(num_bags, self.num_classes).cuda()\n",
        "\n",
        "        # Additional lists to store the saliency_maps, yhat_instances, and attention_scores\n",
        "        saliency_maps, yhat_instances, attention_scores = [], [], []\n",
        "\n",
        "        for j in range(num_bags):\n",
        "            start, end = bag_sizes[j], bag_sizes[j+1]\n",
        "            h_bag = h[start:end]\n",
        "\n",
        "            # Ensure that h_bag has a first dimension of 1 before passing it to the aggregator\n",
        "            h_bag = h_bag.unsqueeze(0)\n",
        "\n",
        "            # Receive four values from aggregator\n",
        "            yhat_bag, sm, yhat_ins, att_sc = self.aggregator(h_bag)\n",
        "\n",
        "            logits[j] = yhat_bag\n",
        "            saliency_maps.append(sm)\n",
        "            yhat_instances.append(yhat_ins)\n",
        "            attention_scores.append(att_sc)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Config\n",
        "    model_name = 'ABMIL'\n",
        "    img_size = 256\n",
        "    batch_size = 4\n",
        "    min_bag_size = 2\n",
        "    max_bag_size = 15\n",
        "    epochs = 15\n",
        "    lr = 0.0008\n",
        "\n",
        "    # Paths\n",
        "    export_location = '/content/drive/MyDrive/CAD_DATASET/CAD_FILES_IMAGES/'\n",
        "    cropped_images = '/content/drive/MyDrive/CAD_DATASET/CAD_FILES_IMAGES/images'\n",
        "    case_study_data = pd.read_csv(f'{export_location}/CaseStudyData.csv')\n",
        "    breast_data = pd.read_csv(f'{export_location}/BreastData.csv')\n",
        "    image_data = pd.read_csv(f'{export_location}/ImageData.csv')\n",
        "\n",
        "\n",
        "    files_train, ids_train, labels_train, files_val, ids_val, labels_val = prepare_all_data(export_location, case_study_data, breast_data, image_data,\n",
        "                                                                                            cropped_images, img_size, min_bag_size, max_bag_size)\n",
        "    #print(\"Training Data...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWAvXRa8BLWV",
        "outputId": "56d9b3c9-8c5f-4115-d4fe-e974e168ae62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing Data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 59428/59428 [00:24<00:00, 2471.55it/s]\n",
            "100%|██████████| 5224/5224 [00:04<00:00, 1112.81it/s]\n",
            "100%|██████████| 1370/1370 [00:01<00:00, 1148.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 32105 files in the training data\n",
            "There are 8403 files in the validation data\n",
            "Number of Malignant Bags: 2264\n",
            "Number of Non-Malignant Bags: 2171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Create datasets\n",
        "    #dataset_train = TUD.Subset(BagOfImagesDataset( files_train, ids_train, labels_train),list(range(0,100)))\n",
        "    #dataset_val = TUD.Subset(BagOfImagesDataset( files_val, ids_val, labels_val),list(range(0,100)))\n",
        "    dataset_train = BagOfImagesDataset(files_train, ids_train, labels_train, img_size)\n",
        "    dataset_val = BagOfImagesDataset(files_val, ids_val, labels_val, img_size)\n",
        "\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dl =  TUD.DataLoader(dataset_train, batch_size=batch_size, collate_fn = collate_custom, drop_last=True, shuffle = True)\n",
        "    val_dl =    TUD.DataLoader(dataset_val, batch_size=batch_size, collate_fn = collate_custom, drop_last=True)\n",
        "\n",
        "    imsize=160\n",
        "    encoder = create_timm_body('resnet18')\n",
        "    nf = num_features_model( nn.Sequential(*encoder.children()))\n",
        "    x = torch.rand(1,3,imsize,imsize) # fake batch of one image to get feature dim\n",
        "    h = encoder(x)\n",
        "    bs, nf, height, width = h.shape\n",
        "    # bag aggregator\n",
        "    use_KSA = True # true to model dependencies between instances\n",
        "    aggregator = KSA_Aggregate( nf, height, width, use_KSA)\n",
        "    #aggregator = ABMIL_aggregate( nf = nf, num_classes = 1, pool_patches = 3, L = 128)\n",
        "    bagmodel = KSA_MultiBagModel(encoder, aggregator).cuda()\n",
        "    optimizer = Adam(bagmodel.parameters(), lr=lr)\n",
        "    loss_func = nn.BCELoss()\n",
        "\n",
        "    train_losses_over_epochs = []\n",
        "    valid_losses_over_epochs = []\n",
        "    all_targs = []\n",
        "    all_preds = []\n",
        "\n"
      ],
      "metadata": {
        "id": "KAIHKsB-HJqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        bagmodel.train()\n",
        "        total_loss = 0.0\n",
        "        total_acc = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for (data, yb) in tqdm(train_dl, total=len(train_dl)):\n",
        "            xb, ids = data\n",
        "            xb, ids, yb = xb.cuda(), ids.cuda(), yb.cuda()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = bagmodel((xb, ids)).squeeze(dim=1)\n",
        "            loss = loss_func(outputs, yb)\n",
        "\n",
        "            #print(f'loss: {loss}\\n pred: {outputs}\\n true: {yb}')\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * len(xb)\n",
        "            predicted = torch.round(outputs).squeeze()\n",
        "            total += yb.size(0)\n",
        "            correct += predicted.eq(yb.squeeze()).sum().item()\n",
        "\n",
        "            if epoch == epochs - 1:\n",
        "                all_targs.extend(yb.cpu().numpy())\n",
        "                if len(predicted.size()) == 0:\n",
        "                    predicted = predicted.view(1)\n",
        "                all_preds.extend(predicted.cpu().detach().numpy())\n",
        "\n",
        "\n",
        "        train_loss = total_loss / total\n",
        "        train_acc = correct / total\n",
        "\n",
        "\n",
        "        # Evaluation phase\n",
        "        bagmodel.eval()\n",
        "        total_val_loss = 0.0\n",
        "        total_val_acc = 0.0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for (data, yb) in tqdm(val_dl, total=len(val_dl)):\n",
        "                xb, ids = data\n",
        "                xb, ids, yb = xb.cuda(), ids.cuda(), yb.cuda()\n",
        "\n",
        "                outputs = bagmodel((xb, ids)).squeeze(dim=1)\n",
        "                loss = loss_func(outputs, yb)\n",
        "\n",
        "                total_val_loss += loss.item() * len(xb)\n",
        "                predicted = torch.round(outputs).squeeze()\n",
        "                total += yb.size(0)\n",
        "                correct += predicted.eq(yb.squeeze()).sum().item()\n",
        "\n",
        "        val_loss = total_val_loss / total\n",
        "        val_acc = correct / total\n",
        "\n",
        "        train_losses_over_epochs.append(train_loss)\n",
        "        valid_losses_over_epochs.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Acc   | Loss\")\n",
        "        print(f\"Train   | {train_acc:.4f} | {train_loss:.4f}\")\n",
        "        print(f\"Val     | {val_acc:.4f} | {val_loss:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    #torch.save(bagmodel.state_dict(), f\"{env}/models/{model_name}.pth\")\n",
        "\n",
        "    # Save the loss graph\n",
        "    #plot_loss(train_losses_over_epochs, valid_losses_over_epochs, f\"{env}/models/{model_name}_loss.png\")\n",
        "\n",
        "    # Save the confusion matrix\n",
        "    #vocab = ['not malignant', 'malignant']  # Replace with your actual vocab\n",
        "    #plot_Confusion(all_targs, all_preds, vocab, f\"{env}/models/{model_name}_confusion.png\")"
      ],
      "metadata": {
        "id": "wbQZV3ERHbGJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}