{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyaMEwEccaFN/sj+nDgdVY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/10Zee/CAD-Project-/blob/main/predict_saliency_maps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw6392nglDJP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from fastai.vision.all import *\n",
        "from torch.nn.functional import binary_cross_entropy\n",
        "from model_ABMIL import *\n",
        "from train_loop import *\n",
        "from data_prep import *\n",
        "import matplotlib.pyplot as plt\n",
        "env = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "\n",
        "\n",
        "def load_trained_model(model_path, encoder_arch):\n",
        "    encoder = create_timm_body(encoder_arch)\n",
        "    nf = num_features_model(nn.Sequential(*encoder.children()))\n",
        "    aggregator = ABMIL_aggregate(nf=nf, num_classes=1, pool_patches=6, L=128)\n",
        "    bagmodel = EmbeddingBagModel(encoder, aggregator).cuda()\n",
        "    bagmodel.load_state_dict(torch.load(model_path))\n",
        "    bagmodel.eval()\n",
        "    return bagmodel\n",
        "\n",
        "\n",
        "def predict_on_test_set(model, test_dl, save_path):\n",
        "    loss_func = nn.BCELoss()\n",
        "\n",
        "    bag_predictions = []\n",
        "    bag_losses = []\n",
        "    bag_ids = []\n",
        "    bag_labels = []\n",
        "    saliency_maps_list = []  # to store saliency maps\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (data, yb, bag_id) in tqdm(test_dl, total=len(test_dl)):\n",
        "            xb, yb = data, yb.cuda()\n",
        "\n",
        "            outputs, saliency_maps, yhat, att = model(xb)\n",
        "            loss = loss_func(outputs, yb)\n",
        "\n",
        "            # Saving the images with saliency maps overlaid\n",
        "            for bag_index, bag_saliency_maps in enumerate(saliency_maps):\n",
        "                for i, saliency in enumerate(bag_saliency_maps):\n",
        "                    # Squeeze the saliency tensor to remove any singleton dimensions\n",
        "                    saliency = saliency.squeeze()\n",
        "\n",
        "                    # Normalize the saliency map for visualization\n",
        "                    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min())\n",
        "\n",
        "                    # Create an empty RGB image with the same size as the saliency map\n",
        "                    saliency_map_rgb = torch.zeros(3, saliency.size(0), saliency.size(1))\n",
        "\n",
        "                    # Place the saliency map in the red channel\n",
        "                    saliency_map_rgb[0] = saliency\n",
        "\n",
        "                    # Convert the RGB saliency map to a PIL image\n",
        "                    saliency_img = TF.to_pil_image(saliency_map_rgb.cpu().detach())\n",
        "\n",
        "                    # Retrieve the corresponding image tensor from the batch\n",
        "                    img_tensor = xb[bag_index][i].cpu()\n",
        "\n",
        "                    # Unnormalize the image tensor to convert it to PIL image\n",
        "                    img = TF.to_pil_image(unnormalize(img_tensor).cpu().detach())\n",
        "\n",
        "                    # Ensure the saliency_img is the same size as the original image\n",
        "                    saliency_img = saliency_img.resize(img.size, Image.BILINEAR)\n",
        "\n",
        "                    # Overlay the red saliency map on the image\n",
        "                    overlayed_img = Image.blend(img, saliency_img, alpha=0.5)\n",
        "\n",
        "                    # Save the overlayed image\n",
        "                    overlayed_img.save(os.path.join(save_path, f'saliency_bag_{bag_id[bag_index]}_img_{i}.png'))\n",
        "\n",
        "\n",
        "            bag_predictions.append(round(outputs.cpu().item(), 4))\n",
        "            bag_losses.append(round(loss.cpu().item(), 4))\n",
        "            bag_ids.append(bag_id[0].cpu().numpy())  # assuming bag_id is a tensor\n",
        "            bag_labels.append(yb.cpu().item())\n",
        "\n",
        "            # Convert each tensor in saliency_maps to numpy and store in saliency_maps_list\n",
        "            saliency_maps_list.extend([s.cpu().numpy() for s in saliency_maps])  # Adjusted this line\n",
        "\n",
        "        return bag_predictions, bag_losses, bag_ids, bag_labels, saliency_maps_list\n",
        "\n",
        "\n",
        "def test_dataset():\n",
        "    test_location = f\"{env}/tests/{model_name}/\"\n",
        "    os.makedirs(test_location, exist_ok=True)\n",
        "\n",
        "    # Load data\n",
        "    bags_train, bags_val = prepare_all_data(export_location, case_study_data, breast_data, image_data, cropped_images, img_size, min_bag_size, max_bag_size)\n",
        "\n",
        "    # Now use the combined data for the dataset\n",
        "    #dataset_combined = TUD.Subset(BagOfImagesDataset( combined_files, combined_ids, combined_labels),list(range(0,100)))\n",
        "    dataset_val = BagOfImagesDataset(bags_val, train=False)\n",
        "    combined_dl = TUD.DataLoader(dataset_val, batch_size=1, collate_fn=collate_custom, drop_last=True)\n",
        "\n",
        "    # Make predictions on test set\n",
        "    predictions, losses, bag_ids, bag_labels, saliency_maps = predict_on_test_set(model, combined_dl, test_location)\n",
        "\n",
        "\n",
        "\n",
        "    # Create a DataFrame to save the results\n",
        "    results_df = pd.DataFrame({\n",
        "        \"Accession_Number\": bag_ids,\n",
        "        \"Prediction\": predictions,\n",
        "        \"True_Label\": bag_labels,\n",
        "        \"Loss\": losses\n",
        "    })\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# Config\n",
        " # Config\n",
        "model_name = 'ABMIL'\n",
        "img_size = 256\n",
        "batch_size = 4\n",
        "min_bag_size = 2\n",
        "max_bag_size = 15\n",
        "epochs = 32\n",
        "lr = 0.0001\n",
        "\n",
        "# Paths\n",
        "export_location = '/content/drive/MyDrive/CAD_DATASET/CAD_FILES_IMAGES/'\n",
        "cropped_images = '/content/drive/MyDrive/CAD_DATASET/CAD_FILES_IMAGES/images'\n",
        "case_study_data = pd.read_csv(f'{export_location}/CaseStudyData.csv')\n",
        "breast_data = pd.read_csv(f'{export_location}/BreastData.csv')\n",
        "image_data = pd.read_csv(f'{export_location}/ImageData.csv')\n",
        "\n",
        "# Load the trained model\n",
        "model_path = f'{env}/models/{model_name}/{model_name}.pth'\n",
        "model = load_trained_model(model_path, encoder_arch)\n",
        "\n",
        "# Test a validation cases\n",
        "results = test_dataset()\n",
        "\n",
        "# Merge df_failed_cases with case_study_data to get the BI-RADS scores\n",
        "merged_data = pd.merge(results, case_study_data[['Accession_Number', 'BI-RADS']], on='Accession_Number', how='left')"
      ]
    }
  ]
}